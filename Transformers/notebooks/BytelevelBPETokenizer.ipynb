{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a6tk3XCzQqo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f51fbf8a-f289-4f25-af9c-8b77b1dc2271"
      },
      "source": [
        "! pip install -q transformers==2.11.0 tokenizers==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 675kB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 22.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 58.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uHZ-Cxq73tA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "963eec54-99e8-42ec-d9f5-a746756ad968"
      },
      "source": [
        "import transformers\n",
        "import tokenizers\n",
        "\n",
        "print(transformers.__version__)\n",
        "print(tokenizers.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.11.0\n",
            "0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8EoOdtT7M3m"
      },
      "source": [
        "with open('text.txt', 'w') as f:\n",
        "    f.write(str('124 abc def'*20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7zBOSny668u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f29503a-d99c-497d-9864-ec45fd29e044"
      },
      "source": [
        "import os\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer, SentencePieceBPETokenizer\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer() # or SentencePieceBPETokenizer()\n",
        "# Get training files\n",
        "paths = os.path.abspath('text.txt')\n",
        "# if not args.train_data_file.endswith(\".txt\"):\n",
        "#     paths = [str(x) for x in Path(paths).glob(\"**/*.txt\")]\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(\n",
        "    files = paths, vocab_size = 50_000,\n",
        "    min_frequency = 3, special_tokens = [\n",
        "                            \"<s>\",\n",
        "                            \"<pad>\",\n",
        "                            \"</s>\",\n",
        "                            \"<unk>\",\n",
        "                            \"<mask>\",\n",
        "                        ]\n",
        ")\n",
        "\n",
        "tokenizer.add_special_tokens([\"<x>\",\"<z>\"])\n",
        "\n",
        "# Save files to disk\n",
        "output_dir = os.path.abspath('tokenizer/')\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "tokenizer.save(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}